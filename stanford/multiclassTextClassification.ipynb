{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import math\n",
    "import numpy\n",
    "from library import terms_to_graph, compute_node_centrality, print_top10\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_train = \"../datasets/webkb-train-stemmed.txt\"\n",
    "path_test = \"../datasets/webkb-test-stemmed.txt\"\n",
    "\n",
    "train = pd.read_csv(path_train, header=None, delimiter='\\t')\n",
    "test = pd.read_csv(path_test, header=None, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train:  (2803, 2)\n",
      "Head of Train: \n",
      "         0                                                  1\n",
      "0  student  brian comput scienc depart univers wisconsin d...\n",
      "1  student  denni swanson web page mail pop uki offic hour...\n",
      "2  faculty  russel impagliazzo depart comput scienc engin ...\n",
      "3  student  dave phd student depart comput scienc univers ...\n",
      "4  project  center lifelong learn design univers colorado ...\n",
      "5  faculty  steve liu associ professor depart comput scien...\n"
     ]
    }
   ],
   "source": [
    "print \"Shape of Train: \",train.shape\n",
    "print \"Head of Train: \"\n",
    "print train.ix[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Test:  (1396, 2)\n",
      "Head of Test: \n",
      "         0                                                  1\n",
      "0  student  eric homepag eric wei tsinghua physic fudan genet\n",
      "1   course  comput system perform evalu model new sept ass...\n",
      "2  student  home page comput scienc grad student ucsd work...\n",
      "3  student  toni web page toni face thing call toni studen...\n",
      "4   course  ec advanc comput architectur credit parallel a...\n",
      "5  faculty  faculti member ci depart research interest par...\n"
     ]
    }
   ],
   "source": [
    "print \"Shape of Test: \",test.shape\n",
    "print \"Head of Test: \"\n",
    "print test.ix[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train:  (2774, 2)\n"
     ]
    }
   ],
   "source": [
    "# get indexes to remove in train\n",
    "index_remove = [ind for ind in range(len(train.ix[:,1])) if (train.ix[ind,1]!=train.ix[ind,1]) or ((train.ix[ind,1]==train.ix[ind,1])and(len(train.ix[ind,1].split(\" \"))<4))]\n",
    "\n",
    "#remove\n",
    "train = train.drop(train.index[index_remove])\n",
    "print \"Shape of Train: \",train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Test:  (1376, 2)\n"
     ]
    }
   ],
   "source": [
    "# get indexes to remove in test and remove\n",
    "index_remove = [ind for ind in range(len(test.ix[:,1])) if (test.ix[ind,1]!=test.ix[ind,1]) or ((test.ix[ind,1]==test.ix[ind,1]) and (len(test.ix[ind,1].split(\" \"))<4))]\n",
    "test = test.drop(test.index[index_remove])\n",
    "print \"Shape of Test: \",test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = train.ix[:,0]\n",
    "unique_labels = list(set(labels))\n",
    "\n",
    "truth = test.ix[:,0]\n",
    "unique_truth = list(set(truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project :  335\n",
      "course :  620\n",
      "student :  1075\n",
      "faculty :  744\n"
     ]
    }
   ],
   "source": [
    "for label in unique_labels:\n",
    "    print label,\": \",len([lab for lab in labels if lab==label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storint terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# storing terms of train documents\n",
    "terms_by_docs = [document.split(\" \") for document in train.ix[:,1]]\n",
    "n_terms_per_doc = [len(terms) for terms in terms_by_docs]\n",
    "\n",
    "# storing terms of test documents\n",
    "terms_by_docs_test = [document.split(\" \") for document in test.ix[:,1]]\n",
    "n_terms_per_doc_test = [len(terms) for terms in terms_by_docs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min terms of train doc:  4\n",
      "Max terms of train doc: 20628\n",
      "Average terms of train doc:  134\n"
     ]
    }
   ],
   "source": [
    "print \"Min terms of train doc: \",min(n_terms_per_doc)\n",
    "print \"Max terms of train doc:\",max(n_terms_per_doc)\n",
    "print \"Average terms of train doc: \",sum(n_terms_per_doc)/len(n_terms_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store all terms\n",
    "all_terms = [term for sublist in terms_by_docs for term in sublist]\n",
    "\n",
    "# compute average number of terms\n",
    "avg_len = sum(n_terms_per_doc)/len(n_terms_per_doc)\n",
    "\n",
    "# unique terms\n",
    "all_unique_terms = list(set(all_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse Document Frequency Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200  terms have been processed\n",
      "400  terms have been processed\n",
      "600  terms have been processed\n",
      "800  terms have been processed\n",
      "1000  terms have been processed\n",
      "1200  terms have been processed\n",
      "1400  terms have been processed\n",
      "1600  terms have been processed\n",
      "1800  terms have been processed\n",
      "2000  terms have been processed\n",
      "2200  terms have been processed\n",
      "2400  terms have been processed\n",
      "2600  terms have been processed\n",
      "2800  terms have been processed\n",
      "3000  terms have been processed\n",
      "3200  terms have been processed\n",
      "3400  terms have been processed\n",
      "3600  terms have been processed\n",
      "3800  terms have been processed\n",
      "4000  terms have been processed\n",
      "4200  terms have been processed\n",
      "4400  terms have been processed\n",
      "4600  terms have been processed\n",
      "4800  terms have been processed\n",
      "5000  terms have been processed\n",
      "5200  terms have been processed\n",
      "5400  terms have been processed\n",
      "5600  terms have been processed\n",
      "5800  terms have been processed\n",
      "6000  terms have been processed\n",
      "6200  terms have been processed\n",
      "6400  terms have been processed\n",
      "6600  terms have been processed\n",
      "6800  terms have been processed\n",
      "7000  terms have been processed\n",
      "7200  terms have been processed\n"
     ]
    }
   ],
   "source": [
    "n_doc = len(labels)\n",
    "\n",
    "idf = dict(zip(all_unique_terms,[0]*len(all_unique_terms)))\n",
    "counter = 0\n",
    "\n",
    "for element in idf.keys():\n",
    "    # number of document for each term appear\n",
    "    df = sum([element in terms for terms in terms_by_docs])\n",
    "    # idf\n",
    "    idf[element] = math.log10(float(n_doc+1)/df)\n",
    "\n",
    "    counter += 1\n",
    "    if counter % 200 == 0:\n",
    "        print counter,\" terms have been processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train & Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "all_graphs = []\n",
    "\n",
    "for terms in terms_by_docs:\n",
    "    g = terms_to_graph(terms,w=3)\n",
    "    all_graphs.append(g)\n",
    "print len(terms_by_docs) == len(all_graphs)\n",
    "print len(set(terms_by_docs[0])) == len(all_graphs[0].vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing vectors representations of each train document\n",
    "b = 0.003\n",
    "features_degree = []\n",
    "features_w_degree = []\n",
    "features_closeness = []\n",
    "features_w_closeness = []\n",
    "features_tfidf = []\n",
    "\n",
    "len_all = len(all_unique_terms)\n",
    "counter = 0\n",
    "idf_keys = idf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 documents have been processed\n",
      "200 documents have been processed\n",
      "300 documents have been processed\n",
      "400 documents have been processed\n",
      "500 documents have been processed\n",
      "600 documents have been processed\n",
      "700 documents have been processed\n",
      "800 documents have been processed\n",
      "900 documents have been processed\n",
      "1000 documents have been processed\n",
      "1100 documents have been processed\n",
      "1200 documents have been processed\n",
      "1300 documents have been processed\n",
      "1400 documents have been processed\n",
      "1500 documents have been processed\n",
      "1600 documents have been processed\n",
      "1700 documents have been processed\n",
      "1800 documents have been processed\n",
      "1900 documents have been processed\n",
      "2000 documents have been processed\n",
      "2100 documents have been processed\n",
      "2200 documents have been processed\n",
      "2300 documents have been processed\n",
      "2400 documents have been processed\n",
      "2500 documents have been processed\n",
      "2600 documents have been processed\n",
      "2700 documents have been processed\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(len(all_graphs)):\n",
    "    graph = all_graphs[i]\n",
    "    terms_in_doc = terms_by_docs[i]\n",
    "    doc_len = len(terms_in_doc)\n",
    "\n",
    "    metrics = compute_node_centrality(graph)\n",
    "\n",
    "    # returns node (1) name, (2) degree, (3) weighted degree, (4) closeness, (5) weighted closeness\n",
    "    feature_row_degree = [0]*len_all\n",
    "    feature_row_w_degree = [0]*len_all\n",
    "    feature_row_closeness = [0]*len_all\n",
    "    feature_row_w_closeness = [0]*len_all\n",
    "    feature_row_tfidf = [0]*len_all\n",
    "\n",
    "    for term in list(set(terms_in_doc)):\n",
    "        index = all_unique_terms.index(term)\n",
    "        idf_term = idf[term]\n",
    "        denominator = (1-b+(b*(float(doc_len)/avg_len)))\n",
    "        metrics_term = [tuple[1:5] for tuple in metrics if tuple[0]==term][0]\n",
    "\n",
    "        # store TW-IDF value\n",
    "        feature_row_degree[index] = (float(metrics_term[0])/denominator) * idf_term\n",
    "        feature_row_w_degree[index] = (float(metrics_term[1])/denominator) * idf_term\n",
    "        feature_row_closeness[index] = (float(metrics_term[2])/denominator) * idf_term\n",
    "        feature_row_w_closeness[index] = (float(metrics_term[3])/denominator) * idf_term\n",
    "\n",
    "        # number of occurences of word in document\n",
    "        tf = terms_in_doc.count(term)\n",
    "\n",
    "        # store TF-IDF value\n",
    "        feature_row_tfidf[index] = ((1+math.log1p(1+math.log1p(tf)))/(1-b+(b*(float(doc_len)/avg_len)))) * idf_term\n",
    "\n",
    "    features_degree.append(feature_row_degree)\n",
    "    features_w_degree.append(feature_row_w_degree)\n",
    "    features_closeness.append(feature_row_closeness)\n",
    "    features_w_closeness.append(feature_row_w_closeness)\n",
    "    features_tfidf.append(feature_row_tfidf)\n",
    "\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        print counter,\"documents have been processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert list of lists into array\n",
    "# documents as rows, terms as colunms ==>> document-terms metrix\n",
    "training_set_tfidf = numpy.array(features_tfidf)\n",
    "training_set_degree = numpy.array(features_degree)\n",
    "training_set_w_degree = numpy.array(features_w_degree)\n",
    "training_set_closeness = numpy.array(features_closeness)\n",
    "training_set_w_closeness = numpy.array(features_w_closeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 'student'), (2, 'student'), (3, 'faculty'), (2, 'student'), (0, 'project'), (3, 'faculty'), (3, 'faculty'), (3, 'faculty'), (2, 'student'), (1, 'course'), (2, 'student'), (3, 'faculty'), (2, 'student'), (2, 'student'), (3, 'faculty'), (3, 'faculty'), (3, 'faculty'), (2, 'student'), (3, 'faculty'), (2, 'student')]\n"
     ]
    }
   ],
   "source": [
    "# convert labels into integers then into column array\n",
    "labels = list(labels)\n",
    "\n",
    "labels_int = [0]*len(labels)\n",
    "for j in xrange(len(unique_labels)):\n",
    "    index_temp = [i for i in range(len(labels)) if labels[i]==unique_labels[j]]\n",
    "    for element in index_temp:\n",
    "        labels_int[element]=j\n",
    "\n",
    "#check that coding smoothly\n",
    "print zip(labels_int,labels)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_array = numpy.array(labels_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "all_graphs_test = []\n",
    "\n",
    "for terms in terms_by_docs_test:\n",
    "    g = terms_to_graph(terms,w=3)\n",
    "    all_graphs_test.append(g)\n",
    "\n",
    "# sanity checks\n",
    "print len(terms_by_docs_test)==len(all_graphs_test)\n",
    "print len(set(terms_by_docs_test[0]))==len(all_graphs_test[0].vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing vector representations of each testing document\n",
      "100 documents have been processed\n",
      "200 documents have been processed\n",
      "300 documents have been processed\n",
      "400 documents have been processed\n",
      "500 documents have been processed\n",
      "600 documents have been processed\n",
      "700 documents have been processed\n",
      "800 documents have been processed\n",
      "900 documents have been processed\n",
      "1000 documents have been processed\n",
      "1100 documents have been processed\n",
      "1200 documents have been processed\n",
      "1300 documents have been processed\n"
     ]
    }
   ],
   "source": [
    "print \"computing vector representations of each testing document\"\n",
    "# each testing document is represented in the training space only\n",
    "\n",
    "features_degree_test = []\n",
    "features_w_degree_test = []\n",
    "features_closeness_test = []\n",
    "features_w_closeness_test = []\n",
    "features_tfidf_test = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in xrange(len(all_graphs_test)):\n",
    "\n",
    "    graph = all_graphs_test[i]\n",
    "\t# retain only the terms originally present in the training set\n",
    "    terms_in_doc = [term for term in terms_by_docs_test[i] if term in all_unique_terms]\n",
    "    doc_len = len(terms_in_doc)\n",
    "\n",
    "    # returns node (1) name, (2) degree, (3) weighted degree, (4) closeness, (5) weighted closeness\n",
    "    metrics = compute_node_centrality(graph)\n",
    "\n",
    "    feature_row_degree_test = [0]*len_all\n",
    "    feature_row_w_degree_test = [0]*len_all\n",
    "    feature_row_closeness_test = [0]*len_all\n",
    "    feature_row_w_closeness_test = [0]*len_all\n",
    "    feature_row_tfidf_test = [0]*len_all\n",
    "\n",
    "    for term in list(set(terms_in_doc)):\n",
    "        index = all_unique_terms.index(term)\n",
    "        idf_term = idf[term]\n",
    "        denominator = (1-b+(b*(float(doc_len)/avg_len)))\n",
    "        metrics_term = [tuple[1:5] for tuple in metrics if tuple[0]==term][0]\n",
    "\n",
    "        # store TW-IDF values\n",
    "        feature_row_degree_test[index] = (float(metrics_term[0])/denominator) * idf_term\n",
    "        feature_row_w_degree_test[index] = (float(metrics_term[1])/denominator) * idf_term\n",
    "        feature_row_closeness_test[index] = (float(metrics_term[2])/denominator) * idf_term\n",
    "        feature_row_w_closeness_test[index] = (float(metrics_term[3])/denominator) * idf_term\n",
    "\n",
    "        # number of occurences of word in document\n",
    "        tf = terms_in_doc.count(term)\n",
    "\n",
    "        # store TF-IDF value\n",
    "        feature_row_tfidf_test[index] = ((1+math.log1p(1+math.log1p(tf)))/(1-0.2+(0.2*(float(doc_len)/avg_len)))) * idf_term\n",
    "\n",
    "    features_degree_test.append(feature_row_degree_test)\n",
    "    features_w_degree_test.append(feature_row_w_degree_test)\n",
    "    features_closeness_test.append(feature_row_closeness_test)\n",
    "    features_w_closeness_test.append(feature_row_w_closeness_test)\n",
    "    features_tfidf_test.append(feature_row_tfidf_test)\n",
    "\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        print counter, \"documents have been processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., document-term matrix)\n",
    "\n",
    "testing_set_degree = numpy.array(features_degree_test)\n",
    "testing_set_w_degree = numpy.array(features_w_degree_test)\n",
    "testing_set_closeness = numpy.array(features_closeness_test)\n",
    "testing_set_w_closeness = numpy.array(features_w_closeness_test)\n",
    "testing_set_tfidf = numpy.array(features_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 'student'), (1, 'course'), (2, 'student'), (2, 'student'), (1, 'course'), (3, 'faculty'), (1, 'course'), (2, 'student'), (3, 'faculty'), (0, 'project'), (2, 'student'), (3, 'faculty'), (2, 'student'), (1, 'course'), (3, 'faculty'), (1, 'course'), (1, 'course'), (2, 'student'), (2, 'student'), (0, 'project')]\n"
     ]
    }
   ],
   "source": [
    "# convert truth into integers then into column array\n",
    "truth = list(truth)\n",
    "\n",
    "truth_int = [0] * len(truth)\n",
    "for j in range(len(unique_truth)):\n",
    "    index_temp = [i for i in range(len(truth)) if truth[i]==unique_truth[j]]\n",
    "    for element in index_temp:\n",
    "        truth_int[element] = j\n",
    "\n",
    "# check that coding went smoothly\n",
    "print zip(truth_int,truth)[:20]\n",
    "\n",
    "truth_array = numpy.array(truth_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy TF-IDF: 0.83648255814\n",
      "accuracy degree: 0.797238372093\n",
      "accuracy w_degree: 0.834302325581\n",
      "accuracy closeness: 0.835755813953\n",
      "accuracy w_closeness: 0.855377906977\n",
      "TFIDF:\n",
      "project: elsevi project utc model architectur focuss network simplifi juli program\n",
      "course: fax decemb system friend wisconsin annual subscrib apach tuth object\n",
      "student: abil accept mail current text engin detail larg septemb web\n",
      "faculty: scienc fateman java creat philadelphia current group lab solut strand\n",
      "\n",
      "degree:\n",
      "project: simplifi utc cartoon fantasi philadelphia comput model arizona program juli\n",
      "course: lab friend subscrib tuth wisconsin apach system decemb annual object\n",
      "student: current text carl web detail engin septemb larg princeton theori\n",
      "faculty: mail human creat current instal lab arizona solut philadelphia strand\n",
      "\n",
      "w_degree:\n",
      "project: network philadelphia elsevi instal man student comput program juli arizona\n",
      "course: final lab friend lectur wisconsin august system decemb annual object\n",
      "student: web arizona histori human current engin carl larg princeton detail\n",
      "faculty: session countri input philadelphia strand lab instal list arizona time\n",
      "\n",
      "closeness:\n",
      "project: arizona project focuss utc model network architectur simplifi juli program\n",
      "course: fax decemb subscrib friend wisconsin system annual apach tuth object\n",
      "student: accept princeton mail text engin current detail larg septemb web\n",
      "faculty: arizona fateman instal java current philadelphia group solut lab strand\n",
      "\n",
      "w_closeness:\n",
      "project: arizona project focuss utc model network architectur simplifi juli program\n",
      "course: fax decemb subscrib friend wisconsin annual system apach tuth object\n",
      "student: email princeton mail text engin current detail larg septemb web\n",
      "faculty: arizona instal fateman java current philadelphia group solut lab strand\n"
     ]
    }
   ],
   "source": [
    "nb_tfidf = MultinomialNB()\n",
    "nb_degree = MultinomialNB()\n",
    "nb_w_degree = MultinomialNB()\n",
    "nb_closeness = MultinomialNB()\n",
    "nb_w_closeness = MultinomialNB()\n",
    "\n",
    "nb_tfidf.fit(training_set_tfidf,labels_array)\n",
    "nb_degree.fit(training_set_degree,labels_array)\n",
    "nb_w_degree.fit(training_set_w_degree,labels_array)\n",
    "nb_closeness.fit(training_set_closeness, labels_array)\n",
    "nb_w_closeness.fit(training_set_w_closeness, labels_array)\n",
    "\n",
    "predictions_tfidf = nb_tfidf.predict(testing_set_tfidf)\n",
    "predictions_degree = nb_degree.predict(testing_set_degree)\n",
    "predictions_w_degree = nb_degree.predict(testing_set_w_degree)\n",
    "predictions_closeness = nb_degree.predict(testing_set_closeness)\n",
    "predictions_w_closeness = nb_w_closeness.predict(testing_set_w_closeness)\n",
    "\n",
    "print \"accuracy TF-IDF:\", accuracy_score(truth_array,predictions_tfidf)\n",
    "print \"accuracy degree:\", accuracy_score(truth_array,predictions_degree)\n",
    "print \"accuracy w_degree:\", accuracy_score(truth_array,predictions_w_degree)\n",
    "print \"accuracy closeness:\", accuracy_score(truth_array,predictions_closeness)\n",
    "print \"accuracy w_closeness:\", accuracy_score(truth_array,predictions_w_closeness)\n",
    "\n",
    "print \"TFIDF:\"\n",
    "print_top10(all_terms,nb_tfidf,unique_labels)\n",
    "print\n",
    "print \"degree:\"\n",
    "print_top10(all_terms,nb_degree,unique_labels)\n",
    "print\n",
    "print \"w_degree:\"\n",
    "print_top10(all_terms,nb_w_degree,unique_labels)\n",
    "print\n",
    "print \"closeness:\"\n",
    "print_top10(all_terms,nb_closeness,unique_labels)\n",
    "print\n",
    "print \"w_closeness:\"\n",
    "print_top10(all_terms,nb_w_closeness,unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVC Clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy TF-IDF: 0.893895348837\n",
      "accuracy degree: 0.904796511628\n",
      "accuracy w_degree: 0.856831395349\n",
      "accuracy closeness: 0.880813953488\n",
      "accuracy w_closeness: 0.909156976744\n",
      "TFIDF:\n",
      "project: juli optic utc read cartoon elsevi page project parallel program\n",
      "course: california fax proof apach object tuth design overlap resourc berkelei\n",
      "student: univers theori lectur text accept degre septemb larg intern detail\n",
      "faculty: peter html polici graduat scienc symposium java lab solut strand\n",
      "\n",
      "degree:\n",
      "project: high confer group model optic utc architectur focuss juli program\n",
      "course: apach fax evalu annual friend amherst system phone object tuth\n",
      "student: document austin current wagner septemb proceed web text detail accept\n",
      "faculty: spoken lab phone html return scienc java lab solut strand\n",
      "\n",
      "w_degree:\n",
      "project: len easi parallel page reserv program read cartoon project elsevi\n",
      "course: exponenti grupen apach friend tuth electron design berkelei overlap resourc\n",
      "student: secur text ask accept html princeton larg degre intern detail\n",
      "faculty: graduat peter game forc section scienc lab java solut strand\n",
      "\n",
      "closeness:\n",
      "project: decemb model page parallel architectur optic juli focuss utc program\n",
      "course: amherst annual friend proof phone berkelei fax apach object tuth\n",
      "student: librari intern larg rajiv proceed web detail text accept septemb\n",
      "faculty: group graduat fly game franc scienc java lab solut strand\n",
      "\n",
      "w_closeness:\n",
      "project: decemb page model parallel architectur juli optic focuss utc program\n",
      "course: amherst friend proof phone annual berkelei fax apach tuth object\n",
      "student: librari intern larg rajiv proceed web detail text accept septemb\n",
      "faculty: creat graduat fly game franc scienc java lab solut strand\n"
     ]
    }
   ],
   "source": [
    "svm_tfidf = svm.LinearSVC()\n",
    "svm_degree = svm.LinearSVC()\n",
    "svm_w_degree = svm.LinearSVC()\n",
    "svm_closeness = svm.LinearSVC()\n",
    "svm_w_closeness = svm.LinearSVC()\n",
    "\n",
    "svm_tfidf.fit(training_set_tfidf,labels_array)\n",
    "svm_degree.fit(training_set_degree,labels_array)\n",
    "svm_w_degree.fit(training_set_w_degree,labels_array)\n",
    "svm_closeness.fit(training_set_closeness, labels_array)\n",
    "svm_w_closeness.fit(training_set_w_closeness, labels_array)\n",
    "\n",
    "predictions_tfidf = svm_tfidf.predict(testing_set_tfidf)\n",
    "predictions_degree = svm_degree.predict(testing_set_degree)\n",
    "predictions_w_degree = svm_degree.predict(testing_set_w_degree)\n",
    "predictions_closeness = svm_degree.predict(testing_set_closeness)\n",
    "predictions_w_closeness = svm_w_closeness.predict(testing_set_w_closeness)\n",
    "\n",
    "print \"accuracy TF-IDF:\", accuracy_score(truth_array,predictions_tfidf)\n",
    "print \"accuracy degree:\", accuracy_score(truth_array,predictions_degree)\n",
    "print \"accuracy w_degree:\", accuracy_score(truth_array,predictions_w_degree)\n",
    "print \"accuracy closeness:\", accuracy_score(truth_array,predictions_closeness)\n",
    "print \"accuracy w_closeness:\", accuracy_score(truth_array,predictions_w_closeness)\n",
    "\n",
    "print \"TFIDF:\"\n",
    "print_top10(all_terms,svm_tfidf,unique_labels)\n",
    "print\n",
    "print \"degree:\"\n",
    "print_top10(all_terms,svm_degree,unique_labels)\n",
    "print\n",
    "print \"w_degree:\"\n",
    "print_top10(all_terms,svm_w_degree,unique_labels)\n",
    "print\n",
    "print \"closeness:\"\n",
    "print_top10(all_terms,svm_closeness,unique_labels)\n",
    "print\n",
    "print \"w_closeness:\"\n",
    "print_top10(all_terms,svm_w_closeness,unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy TF-IDF: 0.917151162791\n",
      "accuracy degree: 0.87863372093\n",
      "accuracy w_degree: 0.849563953488\n",
      "accuracy closeness: 0.877906976744\n",
      "accuracy w_closeness: 0.918604651163\n",
      "TFIDF:\n",
      "project: brad model architectur project juli optic parallel page utc program\n",
      "course: overlap phone california friend proof fax apach berkelei object tuth\n",
      "student: secur proceed wagner theori larg detail web accept text septemb\n",
      "faculty: focuss game graduat symposium html scienc java lab solut strand\n",
      "\n",
      "degree:\n",
      "project: organ network reserv comput architectur utc focuss model juli program\n",
      "course: subscrib phone wisconsin decemb friend apach system annual tuth object\n",
      "student: ask spencer proceed wagner carl septemb web detail accept text\n",
      "faculty: phone html spoken philadelphia return scienc java solut lab strand\n",
      "\n",
      "w_degree:\n",
      "project: architectur cartoon optic juli utc reserv elsevi project page program\n",
      "course: place proof fax resourc friend apach berkelei object overlap tuth\n",
      "student: intern ask librari web larg septemb intern accept text detail\n",
      "faculty: forc graduat nation game phone scienc lab java solut strand\n",
      "\n",
      "closeness:\n",
      "project: parallel organ journal optic model architectur juli focuss utc program\n",
      "course: amherst friend proof berkelei fax phone annual apach object tuth\n",
      "student: current larg rajiv proceed wagner detail web septemb text accept\n",
      "faculty: current return focuss html creat scienc java lab solut strand\n",
      "\n",
      "w_closeness:\n",
      "project: parallel organ journal optic model architectur juli focuss utc program\n",
      "course: amherst friend proof fax berkelei phone annual apach object tuth\n",
      "student: current larg rajiv proceed wagner detail web septemb text accept\n",
      "faculty: current focuss return html creat scienc java lab solut strand\n"
     ]
    }
   ],
   "source": [
    "lgr_tfidf = LogisticRegression()\n",
    "lgr_degree = LogisticRegression()\n",
    "lgr_w_degree = LogisticRegression()\n",
    "lgr_closeness = LogisticRegression()\n",
    "lgr_w_closeness = LogisticRegression()\n",
    "\n",
    "lgr_tfidf.fit(training_set_tfidf,labels_array)\n",
    "lgr_degree.fit(training_set_degree,labels_array)\n",
    "lgr_w_degree.fit(training_set_w_degree,labels_array)\n",
    "lgr_closeness.fit(training_set_closeness, labels_array)\n",
    "lgr_w_closeness.fit(training_set_w_closeness, labels_array)\n",
    "\n",
    "predictions_tfidf = lgr_tfidf.predict(testing_set_tfidf)\n",
    "predictions_degree = lgr_degree.predict(testing_set_degree)\n",
    "predictions_w_degree = lgr_degree.predict(testing_set_w_degree)\n",
    "predictions_closeness = lgr_degree.predict(testing_set_closeness)\n",
    "predictions_w_closeness = lgr_w_closeness.predict(testing_set_w_closeness)\n",
    "\n",
    "print \"accuracy TF-IDF:\", accuracy_score(truth_array,predictions_tfidf)\n",
    "print \"accuracy degree:\", accuracy_score(truth_array,predictions_degree)\n",
    "print \"accuracy w_degree:\", accuracy_score(truth_array,predictions_w_degree)\n",
    "print \"accuracy closeness:\", accuracy_score(truth_array,predictions_closeness)\n",
    "print \"accuracy w_closeness:\", accuracy_score(truth_array,predictions_w_closeness)\n",
    "\n",
    "print \"TFIDF:\"\n",
    "print_top10(all_terms,lgr_tfidf,unique_labels)\n",
    "print\n",
    "print \"degree:\"\n",
    "print_top10(all_terms,lgr_degree,unique_labels)\n",
    "print\n",
    "print \"w_degree:\"\n",
    "print_top10(all_terms,lgr_w_degree,unique_labels)\n",
    "print\n",
    "print \"closeness:\"\n",
    "print_top10(all_terms,lgr_closeness,unique_labels)\n",
    "print\n",
    "print \"w_closeness:\"\n",
    "print_top10(all_terms,lgr_w_closeness,unique_labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Python2.7]",
   "language": "python",
   "name": "conda-env-Python2.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
